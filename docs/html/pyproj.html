<!DOCTYPE html>
<!--comments are weird as shit in html-->
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mateo's Place</title>
    <link rel="stylesheet" href="../main.css">
    <link rel="icon" type="image/x-icon" href="resources/images/hrm.jpg">
</head>
<body>
    <img src="../resources/images/python.png" alt="Python.png" style="float:left;width:5%;height:5%">
    <h1 class = 'title'>Python based projects</h1>
    <hr>
    <div style="background-color:aliceblue"> 
        <table class="pagetable">
            <tr>
                <th class="pageth"><a href = '../index.html' target = '_self' title = 'Home Page'>Home</a></th>
                <th class="pageth"><a href = 'dbproj.html' target = '_self' title = 'Database Projects'>Database-projects</a></th>
                <th class="pageth"><a href = 'jproj.html' target = '_self' title = 'Java Projects'>Java-projects</a></th>
                <th class="pageth"><a href = 'pyproj.html' target = '_self' title = 'Python Projects'>Python-projects</a></th>
                <th class="pageth"><a href = 'c+proj.html' target = '_self' title = 'C++ Projects'>C++-projects</a></th>
            </tr>
        </table>
    </div>
    <h2>Food Bot / Point of Interest</h2>
    <p>Simple project using MapQuest api to suggest food options near a user-specified location that also ended up able to return any point of interest for almost<br>
        any query due to time constraints on the project and it being impractical to limit the query options to only foods. 
    </p>
    <a href = 'https://replit.com/@MateoTopete/Foodbot?v=1' target = '_blank' title = 'Food Bot'>https://replit.com/@MateoTopete/Foodbot?v=1</a>
    <br>
    <h2>Web Scraper and Search Engine</h2>
    <p>Two part course-long project to design a Web scraper to scrape through an internal database of webpages hosted by the school and then create a search engine to search the indexed pages.<br>
    I don't have access to the school network anymore and am unable to retrieve the files containing the indexed webpages to use the search engine, nor can i re-run the scraper to generate a new file <br>
    since the database is only hosted during set project work times and again i no longer can access those resources.
    </p>
    <h3><a href = '../schoolwork/Web Scraper' target = '_blank' title = 'Web Scraper'>The Web Scraper</a></h3>
    <p>The constraints for the Web Scraper were as follows:<br>
    <ul>
        <li>Must be 'polite' - at least 500ms between request to the same domain</li>
        <li>Avoid infinite traps</li>
        <li>Avoid similar pages with little or no new information</li>
        <li>Detect and avoid dead urls</li>
        <li>Detect and avoid crawling very large files</li>
    </ul>
        The Web Scraper took advantage of multi-threading to speed up the long process of crwaling the cache of websites provided. <br>
        Each of the 5 domains we were required to limit our seach to was assigned a mutex so that only one of the 4 threads could make a request at a time <br>
        before waiting the 500ms to release the mutex to ensure politeness. All other tasks such as updating the list of webpages to search next were also made thread-safe<br>
        The html of each webpage was parsed using the <a href = 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/' target = '_blank' title="BeautifulSoup">Beautiful Soup</a> library to read specific html tags for information<br>
    </p>
    <h3><a href = '../schoolwork/Search Engine' target = '_blank' title = 'Search Engine'>The Search Engine</a></h3>
    <p>
        The first part of creating the search engine was making an index using the webpages crawled in the earlier part of the project with the following constraints:
        <ul>
            <li>Must be an inverted index</li>
            <li>Index must be stored as files, not as a database</li>
            <li>Must simulate memory constraints - not allowed to hold entire index in memory at once</li>
            <li>During the creation of the index, it must be offloaded from memory into parts and reconstructed afterwards, again without loading all parts into memory at once</li>
            <li>The amount of webpages indexed is just under 56,000 pages</li>
        </ul>
        Each valid webpage was first tokenized into alphanumeric tokens, then stemmed using Porter Stemming. It was then posted to an index based on the first character of the token. <br>
        Each posting contained the file ID for the posting, the frequency of the word and whether or not it had a bold, heading, or title tag in the html. <br>
        After a certain amount of memory was being used, the indexer would offload and close all sub-indexes (one for each letter) and open up new, blank text files before continuing.<br>
        Upon indexing the final webpage, a merger function is called to merge partial sub-indexes into one final index for each letter.(Parts a1, a2, and a3 merged into an index a containing all postings for 'a' words).<br>
        Each posting in the final index includes a pre-calculated weighted tf-idf score. <br>
        With the index completed, the Search Engine then had the constraints of:
        <ul>
            <li>Doesn't load the entire index into memory at once during a search</li>
            <li>Returns the top 5 most relevant results</li>
            <li>Return results in under 300ms</li>
        </ul>
        The search engine first tokenizes and stems the query so that it matches the postings in the indexes.<br>
        Each page that contains a part of the query is scored, and the top 5 scored pages are returned as the search results.
        Again, unfortunatly i cannot retrieve the indexes i created for the search engine they are stuck on the University's servers or have since been deleted.
    </p>
    <noscript>You need to enable JavaScript to view the full site.</noscript>
</body>
</html>